{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mkgWz-Z22jY"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTemporalFusionTransformer(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self,known_input_dimension, observed_input_dimension, static_input_dimension, hidden_dimension, target_dimension ):\n",
        "      '''known_input_dimension = #Dimension of known input\n",
        "      observed_input_dimension= #Dimension of unknown(observed) input\n",
        "      static_input_dimension=  #Dimension of the static inputs\n",
        "      hidden_dimension = #Chosen dimension of model\n",
        "      target_dimension= # Dimension of target dimension'''\n",
        "      super(SimpleTemporalFusionTransformer, self).__init__()\n",
        "    \n",
        "      \n",
        "      '''Let's assume that the batch x sequence x number of features for each feauture\n",
        "      is the same'''\n",
        "\n",
        "      #Projecting the static input into the hidden dimension\n",
        "      '''We need to project the static input into\n",
        "      the hidden dimension of the model itself:'''\n",
        "      #Dimension of the static inputs\n",
        "      self.static_input_embedding = torch.nn.Linear(static_input_dimension, hidden_dimension)\n",
        "\n",
        "      #Using the LSTM encoder\n",
        "      '''Long short-term memory (LSTM) is an artificial recurrent\n",
        "      neural network (RNN) architecture used in the field of deep \n",
        "      learning\n",
        "      '''\n",
        "\n",
        "      '''This uses both known and unknown inputs'''\n",
        "      self.encoder_lstm = torch.nn.LSTM(\n",
        "      input_size = known_input_dimension +observed_input_dimension,\n",
        "      hidden_size = hidden_dimension,\n",
        "      batch_first= True\n",
        "      )\n",
        "\n",
        "      #Decoding the LSTM\n",
        "      '''Decoding the LSTM would only use\n",
        "      known future inputs of time series data in this case'''\n",
        "      self.decoder_lstm = torch.nn.LSTM(\n",
        "          input_size = known_input_dimension,\n",
        "          hidden_size = hidden_dimension,\n",
        "          batch_first = True\n",
        "      )\n",
        "\n",
        "      #Creating a multi-head attention network\n",
        "      '''We need to create an automated and trainable way to \n",
        "      weight different evaluation metrics accurately'''\n",
        "      self.multi_head_attention = torch.nn.MultiheadAttention(\n",
        "          embed_dim = hidden_dimension,\n",
        "          num_heads = 1,\n",
        "          batch_first= True\n",
        "\n",
        "      )\n",
        "      \n",
        "      '''Output Layer'''\n",
        "      '''This is the outcome\n",
        "       of passing the data through following networks in chronological order:\n",
        "       1) Static data\n",
        "\n",
        "       2)LSTM encoder Layer (s)-------->Multi-Head attention Network\n",
        "\n",
        "       3)LSTM decoder Layer (s)-------->Multi-Head attention Network\n",
        "\n",
        "       4)Output layer'''\n",
        "\n",
        "      self.output_layer = torch.nn.Linear(hidden_dimension,target_dimension)\n",
        "\n",
        "  def forward(self, \n",
        "              past_observed_inputs, \n",
        "              past_known_inputs,\n",
        "              future_known_inputs,static_inputs):\n",
        "    '''\n",
        "    Argument dimensions:\n",
        "    past_observed_inputs = batch size x lookback x features\n",
        "    past_known_inputs = batch size x lookback x feautures\n",
        "    future_known_inputs = batch size  x prediction horizon x features\n",
        "    static_inputs = batch size x features\n",
        "    '''\n",
        "\n",
        "    #Combining input variables\n",
        "    '''Combining all known \n",
        "    and unknown input variables to be passed as static data through the network'''\n",
        "    full_past_inputs = torch.cat((past_observed_inputs,past_known_inputs),axis=-1)\n",
        "    '''We then embed the static inputs into the model dimension'''\n",
        "    embedded_static_inputs= self.static_input_embedding(static_inputs)\n",
        "    '''We must now consider reshaping the embedded static inputs \n",
        "    into the expected hidden state's shape '''\n",
        "    #print(embedded_static_inputs.unsqueeze(0))\n",
        "    encoder_init_hidden_state = [embedded_static_inputs.unsqueeze(0), embedded_static_inputs.unsqueeze(0)]\n",
        "    \n",
        "    ''' Temporal (time_related) embedding of past inputs,\n",
        "    We then initialize the hidden state with embedded static inputs\n",
        "    '''\n",
        "    past_input_temporal_embedding, encoder_hidden_state = self.encoder_lstm(\n",
        "        full_past_inputs,\n",
        "        encoder_init_hidden_state\n",
        "         )\n",
        "    '''\n",
        "    Initialize the hidden state of the decoder lstm.\n",
        "    Here we use the hidden state from the past embedding\n",
        "    '''\n",
        "    future_input_temporal_embedding, decoder_hidden_state = self.decoder_lstm(\n",
        "        future_known_inputs,\n",
        "        encoder_hidden_state\n",
        "         )\n",
        "    '''We can now combine the past and future input embeddings\n",
        "    #Note the dimension of this combination is:\n",
        "    batch size x lookback x prediction horizon x features\n",
        "    '''\n",
        "    combined_temporal_input_embeddings = torch.cat(\n",
        "        (past_input_temporal_embedding, future_input_temporal_embedding),\n",
        "        axis= 1\n",
        "    )\n",
        "    #Passing data into the multi-headed attention later\n",
        "    '''At this stage we need to carefully consider \n",
        "    the weightings of the static decoded data'''\n",
        "    # For this simple case, we have only provided one head\n",
        "    #More heads should be added when we have defined the number of variables\n",
        "    multi_head_attention_output, self.attention_weights = self.multi_head_attention(\n",
        "        combined_temporal_input_embeddings,\n",
        "        combined_temporal_input_embeddings,\n",
        "        combined_temporal_input_embeddings)\n",
        "    '''Extracting the prediction horizons from the \n",
        "    future_known_inputs local variable\n",
        "    \n",
        "    The prediction/ forecast horizon is the length of time into the future for\n",
        "     which forecasts are to be prepared\n",
        "    '''\n",
        "    prediction_horizon = future_known_inputs.shape[1]\n",
        "\n",
        "    '''Checking the multi-headed attention network\n",
        "    and extracting attention outputs'''\n",
        "    future_attention_output = multi_head_attention_output[:,-prediction_horizon:,:]\n",
        "\n",
        "    '''Pass the decoded data through the final output layer'''\n",
        "    final_output = self.output_layer(future_attention_output)\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "PHgW6MpM6Z0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate some fake Data to test this basic neural network\n",
        "'''Here we can construct some fake data to test the network'''\n",
        "#Setting dimentions and (fake) data construction\n",
        "LOOKBACK_HORIZON = 365 #How many days we look back for our observed input variables\n",
        "\n",
        "PREDICTION_HORIZON = 7 #The number of future predictions to make\n",
        "\n",
        "KNOWN_INPUT_DIMENSION = 2 #The dimension of known inputs\n",
        "\n",
        "OBSERVED_INPUT_DIMENSION = 2 #The dimension of known inputs\n",
        "\n",
        "STATIC_INPUT_DIMENSION = 3 #The dimension of static inputs\n",
        "\n",
        "TARGET_DIMENSION = 1 # The dimension of the target variables\n",
        "\n",
        "BATCH_SIZE = 100 #We choose a batch size\n",
        "\n",
        "'''Constructing the data'''\n",
        "#We can use a normal distribution for the sake of ease\n",
        "'''Past Inputs'''\n",
        "past_observed_inputs_data = torch.normal(0,1,(BATCH_SIZE,LOOKBACK_HORIZON, OBSERVED_INPUT_DIMENSION))\n",
        "past_known_inputs_data = torch.normal(0,1,(BATCH_SIZE,LOOKBACK_HORIZON, KNOWN_INPUT_DIMENSION))\n",
        "static_inputs_data = torch.normal(0,1, (BATCH_SIZE,STATIC_INPUT_DIMENSION))\n",
        "'''Future inputs'''\n",
        "future_known_inputs_data = torch.normal(0,1,(BATCH_SIZE,PREDICTION_HORIZON, KNOWN_INPUT_DIMENSION))\n",
        "'''Future targets'''\n",
        "future_targets_data = torch.normal (0,1, (BATCH_SIZE,PREDICTION_HORIZON, TARGET_DIMENSION))"
      ],
      "metadata": {
        "id": "5-MJJQ2k7gWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Constructing the model from the class'''\n",
        "HIDDEN_DIMENSION = 30\n",
        "#print(past_observed_inputs.shape[-1])\n",
        "'''Constructing the model'''\n",
        "\n",
        "model = SimpleTemporalFusionTransformer(known_input_dimension=past_known_inputs_data.shape[-1], #known_input_dimension \n",
        "    observed_input_dimension=past_observed_inputs_data.shape[-1], #observed_input_dimension \n",
        "    static_input_dimension= static_inputs_data.shape[-1], #static_input_dimension \n",
        "    hidden_dimension=HIDDEN_DIMENSION, #hidden_dimension \n",
        "    target_dimension = TARGET_DIMENSION) #target_dimension\n",
        "\n",
        "\n",
        "'''Add an optimizer  using torch.optim'''\n",
        "'''We need to construct an optimizer object, \n",
        "that will hold the current state and will update the parameters \n",
        "based on the computed gradients.\n",
        "\n",
        "Parameters: These are the variables of the model \n",
        "            function and can be called by model.parameters()\n",
        "optimizer-specific options:\n",
        "learning rate = 0.01\n",
        "weight decay\n",
        "\n",
        "General example:\n",
        "optimizer = optim.Adam([var1, var2], lr=0.0001)\n",
        "'''\n",
        "#Optimizer choice:Adam\n",
        "optimizer =torch. optim.Adam(model.parameters(), lr=0.01)\n",
        "'''Adam optimization algorithm is an extension to \n",
        "stochastic gradient descent\n",
        "Blog https://machinelearningmastery.com/adam-optimization-\n",
        "algorithm-for-deep-learning/'''\n",
        "\n",
        "'''Setting a loss function:\n",
        "\n",
        "\n",
        "Loss functions are used to gauge the error\n",
        " between the prediction output and the provided target value. \n",
        " A loss function tells us how far the algorithm model \n",
        "is from realizing the expected outcome.\n",
        "\n",
        "\n",
        "If the deviation is small or the values are \n",
        "nearly identical, it’ll output a very low loss value.\n",
        " Therefore, you need to use a loss function that can penalize \n",
        " a model properly\n",
        " when it is training on the provided dataset.'''\n",
        "\n",
        " #We made need to build our own loss function in the future\n",
        "loss_function = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "w9ki6JFvBaXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"One training Epoch\"\"\"\n",
        "\n",
        "\"\"\"What is an Epoch:\n",
        "An epoch is when the machine learning algorith runs through the training data.\n",
        "One epoch means that each sample in the training dataset has had an opportunity\n",
        "to update the internal model parameters. An epoch is comprised of one or more batches.\n",
        "\n",
        "The number of epochs is a hyperparameter that defines the number times that the \n",
        "learning algorithm will work through the entire training dataset. \"\"\"\n",
        "\n",
        "#What is a batch\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "The batch size is a hyperparameter that defines the number of samples to work\n",
        " through before updating the internal model parameters.\n",
        "\n",
        "Think of a batch as a for-loop iterating over one or more samples and making \n",
        "predictions. At the end of the batch, the predictions are compared to the \n",
        "expected output variables and an error is calculated. From this error, the \n",
        "update algorithm is used to improve the model, e.g. move down along the error\n",
        " gradient.\n",
        "\n",
        "A training dataset can be divided into one or more batches.\n",
        "\n",
        "When all training samples are used to create one batch, the learning algorithm \n",
        "is called batch gradient descent. When the batch is the size of one sample,\n",
        " the learning algorithm is called stochastic gradient descent. When the batch \n",
        " size is more than one sample and less than the size of the training dataset, \n",
        " the learning algorithm is called mini-batch gradient descent.\n",
        "\"\"\"\n",
        "\n",
        "#Training one epoch we can run the model through the model via the 'model.forward()'\n",
        "\n",
        "model_output =model.forward(\n",
        "    past_observed_inputs = past_observed_inputs_data,\n",
        "    past_known_inputs=past_known_inputs_data,\n",
        "    future_known_inputs = future_known_inputs_data,\n",
        "    static_inputs = static_inputs_data)\n",
        "\n",
        "#get the mse loss value\n",
        "'''mse loss'''\n",
        "loss= loss_function(model_output, future_targets_data)\n",
        "\n",
        "#Clear the gradients \n",
        "'''the gradient simply the\n",
        " change in all weights with regard to the change in error (loss)'''\n",
        "optimizer.zero_grad()\n",
        "\n",
        "\"\"\"Propagate the loss backwards to look for change\"\"\"\n",
        "#Backpropagartion\n",
        "loss.backward()\n",
        "\n",
        "\"\"\"Extract gradient descent step\"\"\"\n",
        "#optimization algorithm for finding a local minimum of a differentiable function. \n",
        "#Finds best value\n",
        "optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                            \n"
      ],
      "metadata": {
        "id": "22UfzAfzdKky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947097de-0c6a-427d-9087-c06b3f603767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.4802, -0.4375, -0.3822,  ..., -0.9489,  0.4556, -0.8693],\n",
            "         [-0.1278, -0.7987, -0.0592,  ..., -0.6485,  0.2933, -0.6113],\n",
            "         [ 1.1337,  0.1820,  0.0309,  ..., -1.2703,  0.6236, -0.5927],\n",
            "         ...,\n",
            "         [-0.4725, -0.2154, -0.5835,  ..., -0.3452,  0.3333, -0.3850],\n",
            "         [-1.7562, -0.6300,  0.2834,  ...,  0.3286,  0.0258,  0.4676],\n",
            "         [-0.4070, -0.5647,  0.6057,  ..., -0.4770,  0.2449,  0.0276]]],\n",
            "       grad_fn=<UnsqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Showing the training of the data- visually'''\n",
        "\n",
        "from tqdm import tqdm # training our model with a progress bar\n",
        "'''tqdm is a Python library for adding progress bar. It lets you configure\n",
        " and display a progress bar with metrics you want to track'''\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "'''What is IPython vs Python?\n",
        "IPython is an interactive shell that is built with python. \n",
        "It provides a more useful shell environment to execute python code in\n",
        " REPL (Read Eval Print Loop).'''\n",
        "k=200\n",
        "with tqdm(desc=\"Training Epoch\", total=k) as progress:\n",
        "    for epoch in range(k):\n",
        "        progress.update(1)  # increments the progress bar\n",
        "\n",
        "#for epoch_loop in tdqm(range(200)):\n",
        " # progress.update(1)\n"
      ],
      "metadata": {
        "id": "4fI5nyPzYRz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e816cacc-9547-4639-ce21-c1fd856369e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch: 100%|██████████| 200/200 [00:00<00:00, 479623.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "past_observed_inputs_data\n"
      ],
      "metadata": {
        "id": "dkrwDONaSGFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100f865b-08e2-4cd1-9fa7-b939ff4a895b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.6981,  0.6630],\n",
              "         [-0.9645,  0.3983],\n",
              "         [-0.2106,  0.6476],\n",
              "         ...,\n",
              "         [ 0.8924, -0.7843],\n",
              "         [ 0.9008, -0.9925],\n",
              "         [ 0.3089,  0.7778]],\n",
              "\n",
              "        [[-1.4222,  0.3189],\n",
              "         [ 0.7543, -0.3861],\n",
              "         [ 2.2447, -1.0362],\n",
              "         ...,\n",
              "         [-0.3972, -1.2914],\n",
              "         [-0.6044, -0.4934],\n",
              "         [ 0.9957,  0.7323]],\n",
              "\n",
              "        [[ 0.9141,  1.2325],\n",
              "         [ 0.9302, -0.1430],\n",
              "         [-1.0859, -0.5657],\n",
              "         ...,\n",
              "         [-1.2451,  0.3230],\n",
              "         [ 1.5205,  0.8938],\n",
              "         [ 0.3455,  0.2253]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.6999, -0.2559],\n",
              "         [-0.8430, -0.5155],\n",
              "         [-0.1755,  0.5163],\n",
              "         ...,\n",
              "         [ 0.0453,  1.0072],\n",
              "         [-0.4293,  1.3060],\n",
              "         [-0.1528, -0.3547]],\n",
              "\n",
              "        [[ 0.6810, -0.5732],\n",
              "         [ 0.7327, -0.0349],\n",
              "         [ 1.4500,  1.1789],\n",
              "         ...,\n",
              "         [ 0.6896, -0.7359],\n",
              "         [ 1.0039, -1.6875],\n",
              "         [ 0.4078,  0.3587]],\n",
              "\n",
              "        [[ 0.6747, -0.3173],\n",
              "         [-0.2320, -2.2781],\n",
              "         [ 1.1600, -1.2319],\n",
              "         ...,\n",
              "         [ 1.6600, -1.9700],\n",
              "         [ 0.4513,  0.8550],\n",
              "         [ 0.8670,  0.1412]]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}